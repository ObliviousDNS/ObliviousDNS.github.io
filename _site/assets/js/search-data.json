{"0": {
    "doc": "Datasets",
    "title": "Google Drive Link",
    "content": " ",
    "url": "http://localhost:4000/datasets.html#google-drive-link",
    "relUrl": "/datasets.html#google-drive-link"
  },"1": {
    "doc": "Datasets",
    "title": "Datasets",
    "content": "In an effort to encourage more reproducable network traffic analysis, we have released all datasets used in the original nPrint paper. Each dataset is encoded using pcapML and contains usage and original citation information. ",
    "url": "http://localhost:4000/datasets.html",
    "relUrl": "/datasets.html"
  },"2": {
    "doc": "The nPrint Project",
    "title": "The nPrint Project",
    "content": "The nPrint project is a collection of open source software for network traffic analysis tasks: . | nprint provides a generic representation for machine learning based network traffic analysis | nprintml combines nPrint and automated machine learning techniques for fully automated traffic analysis pipelines | pcapml standardizes traffic analysis tasks at the dataset level by encoding metadata directly into raw traffic captures | pcapml_fe provides a standard, easy to use interface for metadata-encoded traffic captures | . ",
    "url": "http://localhost:4000/",
    "relUrl": "/"
  },"3": {
    "doc": "nPrint",
    "title": "nPrint",
    "content": "nPrint is a standard data representation for network traffic created to be directly usable with machine learning algorithms, replacing feature engineering for a wide array of traffic analysis problems. The original nPrint paper can be found here . ",
    "url": "http://localhost:4000/nprint.html",
    "relUrl": "/nprint.html"
  },"4": {
    "doc": "nPrint",
    "title": "Implemented Protocols",
    "content": "nPrint currently supports the following protocols, but is easily extendable and we encourage users to add to this list! . | IPv4 | IPv6 (Fixed Header) | TCP | UDP | ICMP | Payloads | . ",
    "url": "http://localhost:4000/nprint.html#implemented-protocols",
    "relUrl": "/nprint.html#implemented-protocols"
  },"5": {
    "doc": "nPrint",
    "title": "Citing nPrint",
    "content": "@inproceedings{10.1145/3460120.3484758, author = {Holland, Jordan and Schmitt, Paul and Feamster, Nick and Mittal, Prateek}, title = {New Directions in Automated Traffic Analysis}, year = {2021}, isbn = {9781450384544}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3460120.3484758}, doi = {10.1145/3460120.3484758}, pages = {3366–3383}, numpages = {18}, keywords = {machine learning on network traffic, automated traffic analysis, network traffic analysis}, location = {Virtual Event, Republic of Korea}, series = {CCS '21} } . ",
    "url": "http://localhost:4000/nprint.html#citing-nprint",
    "relUrl": "/nprint.html#citing-nprint"
  },"6": {
    "doc": "Installation",
    "title": "Installation",
    "content": " ",
    "url": "http://localhost:4000/nprint_install.html",
    "relUrl": "/nprint_install.html"
  },"7": {
    "doc": "Installation",
    "title": "Supported Operating Systems",
    "content": ". | Debian Linux | macOS | . ",
    "url": "http://localhost:4000/nprint_install.html#supported-operating-systems",
    "relUrl": "/nprint_install.html#supported-operating-systems"
  },"8": {
    "doc": "Installation",
    "title": "Dependencies",
    "content": ". | libpcap - Packet sniffing | argp - Argument parsing | . Install dependencies on Debian: . sudo apt-get install libpcap-dev . Install dependencies on Mac OS . brew install argp-standalone . ",
    "url": "http://localhost:4000/nprint_install.html#dependencies",
    "relUrl": "/nprint_install.html#dependencies"
  },"9": {
    "doc": "Installation",
    "title": "Install",
    "content": ". | Download the latest release tar here | Extract the tar tar -xvf [nprint-version.tar.gz] | cd [nprint-directory] . | ./configure &amp;&amp; make &amp;&amp; sudo make install | . ",
    "url": "http://localhost:4000/nprint_install.html#install",
    "relUrl": "/nprint_install.html#install"
  },"10": {
    "doc": "Usage",
    "title": "Usage",
    "content": "nPrint can be used in a wide variety of ways. A full (and up to date) list of options can be found by running nprint --help. | Collect traffic in real time and print IPv4 / TCP nPrints to stdout | . nprint -4 -t . | Collect traffic in real time and print IPv4 / TCP nPrints to file | . nprint -4 -t -W test.npt . | Use BPF filters to filter traffic - ICMP nPrints with traffic filtered for only ICMP. | . nprint -i -W test.npt -f icmp . | Read from a PCAP and nPrint the first 20 payload bytes for each packet to an output file | . nprint -P test.pcap -W out.npt -p 20 . | Take a nPrint file and reverse it into a PCAP | . nprint -N test.npt -W test.pcap . | Include a relative timestamp for each packet, capturing timing information along with selected protocols | . nprint -R -4 -t . ",
    "url": "http://localhost:4000/nprint_walk.html",
    "relUrl": "/nprint_walk.html"
  },"11": {
    "doc": "nPrintML",
    "title": "nPrintML",
    "content": "nprintML bridges the gap between nPrint, which generates standard fingerprints for packets, and AutoML, which allows for optimized model training and traffic analysis. nprintML enables users with network traffic and labels to perform optimized packet-level traffic analysis without writing any code. ",
    "url": "http://localhost:4000/nprintml.html",
    "relUrl": "/nprintml.html"
  },"12": {
    "doc": "nPrintML",
    "title": "Citing nPrintML",
    "content": "@inproceedings{10.1145/3460120.3484758, author = {Holland, Jordan and Schmitt, Paul and Feamster, Nick and Mittal, Prateek}, title = {New Directions in Automated Traffic Analysis}, year = {2021}, isbn = {9781450384544}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3460120.3484758}, doi = {10.1145/3460120.3484758}, pages = {3366–3383}, numpages = {18}, keywords = {machine learning on network traffic, automated traffic analysis, network traffic analysis}, location = {Virtual Event, Republic of Korea}, series = {CCS '21} } . ",
    "url": "http://localhost:4000/nprintml.html#citing-nprintml",
    "relUrl": "/nprintml.html#citing-nprintml"
  },"13": {
    "doc": "Docker Demo Image",
    "title": "Docker Demo Image",
    "content": "A Docker container demo is provided for each nPrintML release to aid prospective users in trying out the tool. Note: The container is intended for users of as-yet-unsupported platforms and users of atypically-configured systems. nPrintML should install easily for most users, as described in the preceding section; and, for day-to-day use, it is strongly recommended that nPrintML be installed without virtualization. nPrintML users requiring support in installing the tool should consult the project’s homepage and then consider reaching out for help. Users of unsupported platforms should consider creating a feature request or a pull request. ",
    "url": "http://localhost:4000/nprintml_docker.html",
    "relUrl": "/nprintml_docker.html"
  },"14": {
    "doc": "Docker Demo Image",
    "title": "Dependencies",
    "content": "The container demo requires Docker. Usage . The container entrypoint is the nprintml command, and as such takes the same arguments: . docker run [...] ghcr.io/nprint/nprintml ... Note, however, that any argument references to the host filesystem must be mapped to the container filesystem. For this reason, the nprintml-docker script is recommended. Demo script . The nprintml-docker script is the recommended interface to the container demo. Argument references to the host filesystem are mapped and rewritten for the container filesystem; outputs are written to the host filesystem and given user ownership, etc. nprintml-docker requires Python v3.6 or greater. The script is available for download from the nPrintML repository. It may be placed anywhere on your system, and made executable or invoked with either python or python3. A reasonable installation of the script (globally) might include the following: . DEST=\"/usr/local/bin\" curl --output-dir=\"$DEST\" -O https://raw.githubusercontent.com/nprint/nprintML/main/image/nprintml-docker chmod +x \"$DEST\"/nprintml-docker . The script should then be available for execution, even without reference to its full path, (so long as DEST above is itself in your environment’s PATH): . nprintml-docker ... Note: In the above example, the script is installed to the system globally. This will likely require root permission – e.g. sudo curl … and sudo chmod …. Alternatively, set a different DEST. The script may be placed wherever you have write access – including user directories which are also commonly placed on PATH: $HOME/.local/bin, $HOME/bin, etc. If DEST is not on PATH, the script may be invoked via its full download path, e.g.: . ~/Downloads/nprintml-docker ... And if the script is not made executable, it may be invoked via any available python (v3.6+): . python ~/Downloads/nprintml-docker ... Environment variables . The script’s interface is forwarded to nprintml within the demo container. The script itself may be configured via the following environment variables. | Name | Values | Description | . | NPRINTML_DOCKER_CHOWN | 0, 1* | Outputs owned by root are given user ownership via chown (1) | . | NPRINTML_DOCKER_DEBUG | 0*, 1 | Enable debug-level logging to standard output (1) | . | NPRINTML_DOCKER_REMOVE | 0, 1* | Container is removed after each run (1) | . | NPRINTML_DOCKER_REPOSITORY | (repository: ghcr.io/nprint/nprintml*) | nPrintML repository of image | . | NPRINTML_DOCKER_VERSION | (version: latest*) | nPrintML image version tag | . * default value . ",
    "url": "http://localhost:4000/nprintml_docker.html#dependencies",
    "relUrl": "/nprintml_docker.html#dependencies"
  },"15": {
    "doc": "Installation",
    "title": "Installation",
    "content": "Dependencies . Python versions 3.6 through 3.8 are supported. You might check what versions of Python are installed on your system, e.g.: . ls -1 /usr/bin/python* . As needed, consult your package manager or python.org. Depending on your situation, consider pyenv for easy installation and management of arbitrary versions of Python. nprintML further requires nPrint (see below). Installation . nprintML itself is available for download from the Python Package Index (PyPI) and via pip: . python -m pip install nprintml . This downloads, builds and installs the nprintml console command. If you’re happy to manage your Python (virtual) environment, you’re all set with the above. That said, installation of this command via a tool such as pipx is strongly encouraged. pipx will ensure that nprintML is installed into its own virtual environment, such that its third-party libraries do not conflict with any others installed on your system. (Note that nPrint and nprintML are unrelated to the PyPI distribution named “nprint.”) . Post-installation . nprintML depends on the nPrint command, which may be installed separately. For quick-and-easy satisfaction of this requirement, nprintML supplies the bootstrapping command nprint-install, which is made available to your environment with nprintML installed. This command will inspect its execution environment and attempt to retrieve, compile and install nPrint with the most appropriate defaults: . nprint-install . nPrint may thereby be installed system-globally, to the user environment, to the (virtual) environment to which nprintML was installed, or to a specified path prefix. Consult the command’s --help for more information. nprint-install is identically available through its Python module (no different from pip above): . python -m nprintml.net.install . Further set-up . nprintML leverages AutoGluon to manage AutoML. However, it does not by default install additional libraries required for all models supported by AutoGluon. If you wish to test these models, you will need to install their requirements manually. AutoGluon will itself note which models it is unable to generate – and how to satisfy their requirements – during operation. For more information, consult the AutoGluon documentation. ",
    "url": "http://localhost:4000/nprintml_install.html",
    "relUrl": "/nprintml_install.html"
  },"16": {
    "doc": "Usage",
    "title": "Usage",
    "content": " ",
    "url": "http://localhost:4000/nprintml_walk.html",
    "relUrl": "/nprintml_walk.html"
  },"17": {
    "doc": "Usage",
    "title": "Label Files",
    "content": "Label files must always conform to the same CSV structure. The optional header of the CSV is Item,Label (case-insensitive). An example label file for a list of pcaps is below. Item,Label # Optional header anomaly_1.pcap,anomaly benign_2.pcap,benign benign_2.pcap,benign anomaly_2.pcap,anomaly anomaly_3.pcap,anomaly ... ",
    "url": "http://localhost:4000/nprintml_walk.html#label-files",
    "relUrl": "/nprintml_walk.html#label-files"
  },"18": {
    "doc": "Usage",
    "title": "Directory Usage",
    "content": "nPrintML allows users to supply a directory of PCAPs and a label file to create a full traffic analysis pipeline. Each single PCAP is considered a single sample for purposes of machine learning. Using the above label file as labels.csv and the directory pcaps/, we can run nPrintML on the entire directory in a single call. For this example, we include IPv4 and TCP headers in the nPrints. nprintml -a pcap --pcap_dir pcaps/ -L labels.csv -4 -t . By default, nprintML will automatically pad every sample with blank nPrints to the maximum PCAP size in the list of labeled fies. This can be avoided with the -c flag to use only the first c packets in each labeled pcap. nprintml -a pcap --pcap_dir pcaps/ -L labels.csv -4 -t -c 25 . To summarize, the above command will: . | run nprint on every pcap in the directory with the supplied arguments | load all nPrints into memory | pad samples to the maximum number of packets found in a single nPrint | attach a label to each nPrint | train and evaluate a model on the nPrints. | . ",
    "url": "http://localhost:4000/nprintml_walk.html#directory-usage",
    "relUrl": "/nprintml_walk.html#directory-usage"
  },"19": {
    "doc": "Usage",
    "title": "Single PCAP Usage",
    "content": "nPrintML can also create full traffic analysis pipelines from a single PCAP and a list of labels. To do so, we need to understand nprint indexes. nPrint Indexes . every nPrint is a CSV file with a specific index. This allows us to load nPrints as dataframes and join labels on the index of the nPrint. By default, the index of each nPrint file is the source IP of each packet in the file. Other options include source ports, destination ports, and traffic flows (5-tuples). The full list of index options is below. 0: source IP (default) 1: destination IP 2: source port 3: destination port 4: flow (5-tuple) . Single PCAP labels . When using nprintML in single-pcap mode, we need to supply a label file that maps the index of each nPrint to a label. For example, if we are using the default source IP index, the label file could look like the one below. Item,Label # Optional header 1.2.3.4,device_type1 2.3.4.5,device_type1 4.5.6.7,device_type2 5.6.7.8,device_type3 ... Single PCAP options . By default, nPrintML will perform per-packet machine learning, meaning every packet is a single sample. An example of this is below. nPrintml -a index -L labels.txt -P traffic.pcap -4 -t . In many cases, we may want to train models on sequences of packets. This is possible using the --sample_size argument of nprintML. This will aggregate packets of the same item in sample_size groups (in timeseries order). nPrintml -a index -L labels.txt -P traffic.pcap -4 -t --sample_size 10 . ",
    "url": "http://localhost:4000/nprintml_walk.html#single-pcap-usage",
    "relUrl": "/nprintml_walk.html#single-pcap-usage"
  },"20": {
    "doc": "pcapML",
    "title": "pcapml",
    "content": "pcapML is a system for improving the reproducability of traffic analysis tasks. pcapML leverages the pcapng file format to encode metadata directly into raw traffic captures, removing any ambiguity about which packets belong to any given traffic flow, application, attack, etc., while still being compatiable with popular tools such as tshark and tcpdump. For dataset curators, pcapML provides an easy way to encode metadata into raw traffic captures, ensuring the dataset is used in a consistent manner. On the analysis side, pcapML provides a standard dataset format for users to work with across different datasets. ",
    "url": "http://localhost:4000/pcapml.html#pcapml",
    "relUrl": "/pcapml.html#pcapml"
  },"21": {
    "doc": "pcapML",
    "title": "pcapML",
    "content": " ",
    "url": "http://localhost:4000/pcapml.html",
    "relUrl": "/pcapml.html"
  },"22": {
    "doc": "pcapML_FE",
    "title": "pcapML_FE",
    "content": "The ultimate goal of many traffic analysis tasks is to extract information from a set of packets to identify an object, such as an application. pcapml creates a standard format for researchers to interface with using standard traffic analysis tools. To facilitate faster and simpler traffic analysis pipelines, we’ve created pcapML_FE (Feature Explorer), which enables researchers to focus their efforts on new methods of information extraction* as opposed to dataset interaction, parsing, and metadata attachment. pcapml_fe interacts directly with pcapml encoded datasets, exposing an iterator over traffic samples and their associated metadata. ",
    "url": "http://localhost:4000/pcapml_fe.html",
    "relUrl": "/pcapml_fe.html"
  },"23": {
    "doc": "Installation",
    "title": "Installation",
    "content": "pcapML_FE can be installed using pip or from source. ",
    "url": "http://localhost:4000/pcapml_fe_install.html",
    "relUrl": "/pcapml_fe_install.html"
  },"24": {
    "doc": "Installation",
    "title": "Pip",
    "content": "pip install pcapml-fe . ",
    "url": "http://localhost:4000/pcapml_fe_install.html#pip",
    "relUrl": "/pcapml_fe_install.html#pip"
  },"25": {
    "doc": "Installation",
    "title": "Install From Source",
    "content": "Supported Operating Systems . | Debian Linux | macOS | . Dependencies . | libpcap - Packet sniffing | python3-dev - Header files and library for Python | argp - Argument parsing | . Install dependencies on Debian: . sudo apt-get install libpcap-dev python3-dev . Install dependencies on Mac OS . brew install argp-standalone . Installation . | clone repository: git clone [pcapml_fe] | move to pcapML_FE directory: cd pcapml_fe | update pcapML submodule: git submodule update --init --recursive | install pcapML_FE module: python src/setup.py install | . ",
    "url": "http://localhost:4000/pcapml_fe_install.html#install-from-source",
    "relUrl": "/pcapml_fe_install.html#install-from-source"
  },"26": {
    "doc": "Usage",
    "title": "Usage",
    "content": "pcapml_fe exposes a simple iterator over a sorted pcapml encoded dataset. The iterator can return raw bytes, scapy packets or dpkt packets. The iterator contains all of the information about a single traffic sample, including the sampleID, the metadata associated with the traffic, the raw packet buffers, and the timestamps associated with each raw packet. Below is a full example (source available here) of loading and iterating over the pcapml encoded DTLS dataset available for download . Note: The third import provides helpers that can read raw bytes into dpkt or scapy Ethernet frames (dpkt_readEther and scapy_readEther, respectively). The helpers are not strictly necessary to parse pcapML files. import argparse import pcapml_fe from pcapml_fe_helpers import * def main(): ''' Reads a pcapng file labeled and sorted with pcapml, presenting traffic samples to the user for features to be extracted from. To test the method on a new dataset the only needed change is to load in a different dataset ''' parser = argparse.ArgumentParser() parser.add_argument('pcapml_dataset') args = parser.parse_args() for traffic_sample in pcapml_fe.sampler(args.pcapml_dataset): extract_info(traffic_sample) def extract_info(traffic_sample): ''' Each sample contains the sampleID, metadata and a list of packets with their associated timestamps ''' print(\"Sample ID:\", traffic_sample.sid) print(\"Sample metadata:\", traffic_sample.metadata) ''' iterating over the traffic sample (packets and timestamps) ''' for pkt in traffic_sample.packets: # Print packet timestamp and raw bytes print(pkt.ts, pkt.raw_bytes) dpacket = dpkt_readEther(pkt.raw_bytes) dpacket.pprint() spacket = scapy_readEther(pkt.raw_bytes) print(spacket.summary()) pass if __name__ == '__main__': main() . ",
    "url": "http://localhost:4000/pcapml_fe_walk.html",
    "relUrl": "/pcapml_fe_walk.html"
  },"27": {
    "doc": "Installation",
    "title": "Installation",
    "content": " ",
    "url": "http://localhost:4000/pcapml_install.html",
    "relUrl": "/pcapml_install.html"
  },"28": {
    "doc": "Installation",
    "title": "Supported Operating Systems",
    "content": ". | Debian Linux | macOS | . ",
    "url": "http://localhost:4000/pcapml_install.html#supported-operating-systems",
    "relUrl": "/pcapml_install.html#supported-operating-systems"
  },"29": {
    "doc": "Installation",
    "title": "Dependencies",
    "content": ". | libpcap - Packet sniffing | argp - Argument parsing | . Install dependencies on Debian: . sudo apt-get install libpcap-dev . Install dependencies on Mac OS . brew install argp-standalone . ",
    "url": "http://localhost:4000/pcapml_install.html#dependencies",
    "relUrl": "/pcapml_install.html#dependencies"
  },"30": {
    "doc": "Installation",
    "title": "Installation",
    "content": ". | Download the latest release tar here | Extract the tar tar -xvf [pcapml-version.tar.gz] | cd [pcapml-directory] . | ./configure &amp;&amp; make &amp;&amp; sudo make install | . ",
    "url": "http://localhost:4000/pcapml_install.html",
    "relUrl": "/pcapml_install.html"
  },"31": {
    "doc": "Usage",
    "title": "Walkthrough",
    "content": " ",
    "url": "http://localhost:4000/pcapml_walk.html#walkthrough",
    "relUrl": "/pcapml_walk.html#walkthrough"
  },"32": {
    "doc": "Usage",
    "title": "Overview",
    "content": "pcpaml standardizes network traffic analysis tasks at the dataset level. Rather than focus on a standardized methodology, feature set, or library for combining traffic traces and metadata (such as labels for machine learning tasks), pcapml provides a system for directly coupling raw traffic traces and metadata by using the Next Generation PCAP (pcpang) format. pcapng files can still be read by libraries such as libpcap, and inspected using tools such as tcpdump or tshark. Whereas a pcap represents a linked-list of packets, a pcapng represents a linked list of blocks, which we can use to directly couple metadata and raw packets. Sample IDs . pcapml attaches a sampleID to each packet, enabling us to group packets arbitrarily. With arbitrary packet groupings, we can attach metadata to any set of packets, such as a traffic flow, device, application, anomaly, or time window. a sampleID is created by hashing the metadata associated with a given packet. ",
    "url": "http://localhost:4000/pcapml_walk.html#overview",
    "relUrl": "/pcapml_walk.html#overview"
  },"33": {
    "doc": "Usage",
    "title": "Usage",
    "content": "PcapML Directory Mode . pcapml can attach a sample ID and metadata to a directory of pcaps, one traffic sample per pcap file. Let’s walk through an example of this usage type using the snowflake fingerprintability dataset. This dataset contains a set of DTLS handshakes from four applications, Facebook Messenger, Discord, Google Hangouts, and Snowflake. Each handshake was gathered to understand if Snowflake could be uniqeuly identified from the other services. Directory Metadata Files . When using pcapml in directory mode, metadata files are expected to conform to a simple CSV structure. pcapml expects no header on the CSV file, but will skip any line beginnning with a #. Each line is expected to be in filepath,metadata format. An example metadata file for the snowflake fingeprintability dataset is shown below. facebook-handshake-1.pcap,facebook discord-handshake-1.pcap,discord discord-handshake-2.pcap,discord snowflake-handshake-1.pcap,snowflake ..... Directory Usage . We can then attach the metadata for each handshake in the dataset with a unique sampleID and it’s corresponding application metadata in a pcapng with using pcapml in a single command. $ pcapml -D dataset/ -L metadata.csv -W snowflake-dataset.pcapng . This results in a pcapng that can be examined with tcpdump. $ tcpdump -r snowflake-dataset.pcapng -c 10 reading from file dtls-dataset.pcapng, link-type EN10MB (Ethernet) 12:58:52.562021 IP 74.125.250.71.19305 &gt; 192.168.7.222.55937: UDP, length 161 12:58:52.562788 IP 192.168.7.222.55937 &gt; 74.125.250.71.19305: UDP, length 618 12:58:52.585452 IP 74.125.250.71.19305 &gt; 192.168.7.222.55937: UDP, length 1119 12:58:52.586333 IP 192.168.7.222.55937 &gt; 74.125.250.71.19305: UDP, length 962 13:07:34.459150 IP 74.125.250.26.19305 &gt; 192.168.7.222.54537: UDP, length 161 13:07:34.460771 IP 192.168.7.222.54537 &gt; 74.125.250.26.19305: UDP, length 617 13:07:34.486225 IP 74.125.250.26.19305 &gt; 192.168.7.222.54537: UDP, length 1119 13:07:34.487034 IP 192.168.7.222.54537 &gt; 74.125.250.26.19305: UDP, length 962 17:12:42.435787 IP 74.125.250.71.19305 &gt; 192.168.7.222.54510: UDP, length 161 17:12:42.438214 IP 192.168.7.222.54510 &gt; 74.125.250.71.19305: UDP, length 705 . Upon further inspection using tshark, we see the sampleID and met directly encoded in the output file, where each handshake receives a unique sampleID, leaving no ambiguity on how the metadata is attached to the traffic. $ tshark -r snowflake-dataset.pcapng -T fields -E header=y -e frame.comment -c 10 9003219589747928972,google 9003219589747928972,google 9003219589747928972,google 9003219589747928972,google 18186043603218801379,google 18186043603218801379,google 18186043603218801379,google 18186043603218801379,google 14792257769479651673,google 14792257769479651673,google . PcapML Single Pcap Mode . pcapml can attach a sample ID and metadata to traffic in a single pcap by leveraging BPF filters, time windows, or any combination of the two. Single PCAP Metadata Files . When using pcapml in single PCAP mode, metadata files are expected to conform to a simple CSV structure. pcapml expects no header on the CSV file, but will skip any line beginnning with a #. Each line is expected to be in Metadata,BPF_filter,Timestamp_start,Timestamp_end format. Any of these fields can be left blank that are not in use. For example, if we wanted to attach a piece of metadata to every packet from a few IP addresses in a given traffic capture, the metadata file may resemble the one below. windows_device,src 1.2.3.4,, mac_device,src 5.6.7.8,, linux_device,src 4.3.2.1,, ... If we wanted to only attach metadata to packets from that IP address in a given time frame we could combine any BPF filter with the timestamp options. windows_device,src 1.2.3.4,2345678,2346789 mac_device,src 5.6.7.8,, linux_device,src 4.3.2.1,, ... Finally, if we wanted to attach metadata to traffic in an arbitrary time window, say an anomaly, we can simply not supply a BPF filter for the metadata to be attached. anomaly,,2345678,2346789 anomaly,,3456789,4567890 benign,,,1234567,1234578 ... Single PCAP Usage . We can then attach the metadata for each line in our metadata file in a pcapng using pcapml in a single command. $ pcapml -P traffic.pcap -L metadata.csv -W encoded-dataset.pcapng . PcapML Sorting . By default, any pcapml encoded file that is encoded using single pcap mode is left in the original order the traffic was capture in. In many cases, it is beneficial to instead group the packets first by the sampleID that they are associated with and then in time order. pcapml can sort the packets of any pcapml encoded dataset by sampleID -&gt; timestamp in the same command that metadata encoding is performed. $ pcapml -P traffic.pcap -L metadata.csv -W encoded-dataset.pcapng -s . PcapML Extraction Mode . pcapml can transform any pcapng file encoded using pcapml into a directory of pcap files, one file per sampleID, using a single command. $ pcapml -M snowflake-dataset.pcapng -O output_dir/ . The associated output directory is below. 12868490791586055289_google.pcap 1567624542436120405_google.pcap 1912376493094597460_facebook.pcap 4676138587463220727_discord.pcap 7254850485921062848_snowflake.pcap 9982255779078537418_facebook.pcap 12869046855586552312_google.pcap 15679484754686191639_snowflake.pcap 1913144610008489287_snowflake.pcap 4681195497095943526_google.pcap 7256930666031261978_facebook.pcap 9985167304055034928_discord.pcap 1286997930834027255_snowflake.pcap 15679812277643271301_facebook.pcap 1916232746973137357_discord.pcap 4681553453692518285_firefox_facebook.pcap 7259083765404759561_google.pcap 9987359559073017848_discord.pcap 12872114975008852282_google.pcap 15680488968942900640_discord.pcap 1923291248933451411_google.pcap 4682231651136175711_firefox_snowflake.pcap 7270091391588454401_facebook.pcap 9987474006825771582_discord.pcap 1287296956060682578_snowflake.pcap 15684992164591678892_discord.pcap 1925192065339906372_discord.pcap 4683353161259521763_chrome_discord.pcap 7275055456267078471_discord.pcap 9988384164514239661_discord.pcap 12873202627492535975_facebook.pcap 15686837623379429946_snowflake.pcap 1926070980564693651_facebook.pcap 4686331860154165481_chrome_google.pcap 7275947196122656266_facebook.pcap 9988671347025180223_google.pcap metadata.csv . Also note that a metadata.csv file is generated which maps each individual pcap to the metadata associated with the traffic in that file. $ head metadata.csv . File,Metadata 14944434813179707824_google.pcap,google 14395580548679227705_google.pcap,google 14489979562741152699_google.pcap,google 870078443570293459_google.pcap,google 6809604472343037417_google.pcap,google 9649013506394351716_google.pcap,google 16984261106149530861_google.pcap,google 12493399449979137519_google.pcap,google 7073271527767585992_google.pcap,google . ",
    "url": "http://localhost:4000/pcapml_walk.html",
    "relUrl": "/pcapml_walk.html"
  },"34": {
    "doc": "Usage",
    "title": "Analysis",
    "content": "Although pcapml output can be read by tools such as tshark or tcpudmp, we realize that the crux of traffic analysis tasks involves extracting identifying information from traffic samples. As such, we have built and released pypcapML to easily and directly interact with pcapml encoded datasets. ",
    "url": "http://localhost:4000/pcapml_walk.html#analysis",
    "relUrl": "/pcapml_walk.html#analysis"
  },"35": {
    "doc": "Publications",
    "title": "Publications",
    "content": ". | New Directions In Automated Traffic Analysis . | Paper . | ACM CCS 2021 | ArXiv | . | Presentations . | ACM CCS 2021 . | Short Video | Short Slides | Long Video | Long Slides | . | NANOG . | Video | . | . | BibTex Citation @inproceedings{10.1145/3460120.3484758, author = {Holland, Jordan and Schmitt, Paul and Feamster, Nick and Mittal, Prateek}, title = {New Directions in Automated Traffic Analysis}, year = {2021}, isbn = {9781450384544}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3460120.3484758}, doi = {10.1145/3460120.3484758}, booktitle = {Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security}, pages = {3366–3383}, numpages = {18}, keywords = {automated traffic analysis, machine learning on network traffic, network traffic analysis}, location = {Virtual Event, Republic of Korea}, series = {CCS '21} } . | . | . ",
    "url": "http://localhost:4000/publications.html",
    "relUrl": "/publications.html"
  }
}
